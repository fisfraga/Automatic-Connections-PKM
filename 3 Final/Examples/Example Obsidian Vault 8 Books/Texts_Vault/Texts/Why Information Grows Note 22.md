




---

tags: #Highlight

author: [Cesar Hidalgo]

---
# Why Information Grows Note 22




So in  [[Claude Shannon|Shannon]] ’s language, information and  [[Entropy]]  are functionally equivalent because the number of bits you need to specify a message ( [[Claude Shannon|Shannon]] ’s information) is a function of the number of possible messages that could be transmitted (the  [[Multiplicity (mathematics)|multiplicity]]  of states, which we know as  [[Entropy]] ).


## Recommended Highlights

### Semantic Relatedness


- sameauthor1 [[Why Information Grows Note 23]] - 0.688

- sameauthor2 [[Why Information Grows Note 20]] - 0.675

- diffauthor1 [[Building a Second Brain Note 10]] - 0.45

- diffauthor2 [[The PARA Method Note 9]] - 0.238
### Shared Concept Relatedness


- sameauthor1 [[Why Information Grows Note 21]] - 0.722

- sameauthor2 [[Why Information Grows Note 33]] - 0.562

- diffauthor1 [[The PARA Method Note 36]] - 0.403

- diffauthor2 [[Building a Second Brain Note 11]] - 0.381
# Metadata


- frombook [[Why Information Grows]]

- fromauthor [[Cesar Hidalgo]]

- hasconcept [[Entropy]]

- hasconcept [[Multiplicity (mathematics)]]

- hasconcept [[Claude Shannon]]
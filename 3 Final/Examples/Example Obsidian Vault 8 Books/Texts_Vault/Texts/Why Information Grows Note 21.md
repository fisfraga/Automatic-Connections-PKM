




---

tags: #Highlight

author: [Cesar Hidalgo]

---
# Why Information Grows Note 21




It is worth noting that  [[Entropy]] , even though commonly associated with  [[Randomness|disorder]] , is not exactly a measure of  [[Randomness|disorder]] .  [[Entropy]]  measures the  [[Multiplicity (mathematics)|multiplicity]]  of a  [[Quantum state|state]]  (the number of  [[Quantum state|states]]  that are equivalent). It so happens, however, that  [[Randomness|disordered]]   [[Quantum state|states]]  tend to have high  [[Multiplicity (mathematics)|multiplicity]] , so in practice high- [[Entropy]]   [[Quantum state|states]]  are extremely likely to be disordered. That is why equating  [[Randomness|disorder]]  with  [[Entropy]]  is not such a bad simplification. Yet  [[Entropy]]  can increase without increasing  [[Randomness|disorder]] .


## Recommended Highlights

### Semantic Relatedness


- sameauthor1 [[Why Information Grows Note 23]] - 0.57

- sameauthor2 [[Why Information Grows Note 2]] - 0.568

- diffauthor1 [[The PARA Method Note 9]] - 0.245

- diffauthor2 [[The PARA Method Note 41]] - 0.237
### Shared Concept Relatedness


- sameauthor1 [[Why Information Grows Note 22]] - 0.722

- sameauthor2 [[Why Information Grows Note 23]] - 0.707

- diffauthor1 [[Building a Second Brain Note 36]] - 0.423

- diffauthor2 [[The PARA Method Note 36]] - 0.368
# Metadata


- frombook [[Why Information Grows]]

- fromauthor [[Cesar Hidalgo]]

- hasconcept [[Randomness]]

- hasconcept [[Entropy]]

- hasconcept [[Quantum state]]

- hasconcept [[Multiplicity (mathematics)]]
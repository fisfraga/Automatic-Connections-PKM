




---

tags: #Highlight

author: [Cesar Hidalgo]

---
# Why Information Grows Note 21




It is worth noting that  [[Entropy]] , even though commonly associated with  [[Randomness|disorder]] , is not exactly a measure of  [[Randomness|disorder]] .  [[Entropy]]  measures the  [[Multiplicity (mathematics)|multiplicity]]  of a  [[Quantum state|state]]  (the number of  [[Quantum state|states]]  that are equivalent). It so happens, however, that  [[Randomness|disordered]]   [[Quantum state|states]]  tend to have high  [[Multiplicity (mathematics)|multiplicity]] , so in practice high- [[Entropy]]   [[Quantum state|states]]  are extremely likely to be disordered. That is why equating  [[Randomness|disorder]]  with  [[Entropy]]  is not such a bad simplification. Yet  [[Entropy]]  can increase without increasing  [[Randomness|disorder]] .


## Recommended Highlights

### Semantic Relatedness


- sameauthor1 [[Why Information Grows Note 23]] - 0.57

- sameauthor2 [[Why Information Grows Note 2]] - 0.568

- diffauthor1 [[The Knowledge Illusion Note 4]] - 0.199

- diffauthor2 [[The Knowledge Illusion Note 13]] - 0.182
### Shared Concept Relatedness


- sameauthor1 [[Why Information Grows Note 22]] - 0.709

- sameauthor2 [[Why Information Grows Note 23]] - 0.686

- diffauthor1 [[The Knowledge Illusion Note 5]] - 0.224

- diffauthor2 [[The Knowledge Illusion Note 7]] - 0.201
# Metadata


- frombook [[Why Information Grows]]

- fromauthor [[Cesar Hidalgo]]

- hasconcept [[Randomness]]

- hasconcept [[Entropy]]

- hasconcept [[Quantum state]]

- hasconcept [[Multiplicity (mathematics)]]





---

tags: #Highlight

author: [Cesar Hidalgo]

---
# Why Information Grows Note 22




So in  [[Claude Shannon|Shannon]] ’s language, information and  [[Entropy]]  are functionally equivalent because the number of bits you need to specify a message ( [[Claude Shannon|Shannon]] ’s information) is a function of the number of possible messages that could be transmitted (the  [[Multiplicity (mathematics)|multiplicity]]  of states, which we know as  [[Entropy]] ).


## Recommended Highlights

### Semantic Relatedness


- sameauthor1 [[Why Information Grows Note 23]] - 0.688

- sameauthor2 [[Why Information Grows Note 20]] - 0.675

- diffauthor1 [[The Knowledge Illusion Note 14]] - 0.191

- diffauthor2 [[The Knowledge Illusion Note 6]] - 0.179
### Shared Concept Relatedness


- sameauthor1 [[Why Information Grows Note 21]] - 0.709

- sameauthor2 [[Why Information Grows Note 33]] - 0.541

- diffauthor1 [[The Knowledge Illusion Note 12]] - 0.206

- diffauthor2 [[The Knowledge Illusion Note 14]] - 0.199
# Metadata


- frombook [[Why Information Grows]]

- fromauthor [[Cesar Hidalgo]]

- hasconcept [[Entropy]]

- hasconcept [[Multiplicity (mathematics)]]

- hasconcept [[Claude Shannon]]